from datetime import datetime
from airflow.models import DAG
from airflow.operators.bash import BashOperator
from airflow.hooks.base_hook import BaseHook


connection = BaseHook.get_connection("postgres_connection_main")

default_args = {
    "owner": "etl_user",
    "depends_on_past": False,
    "start_date": datetime(2024, 9, 27),
}

dag = DAG('dag2', default_args=default_args, schedule_interval='0 * * * *', catchup=True,
          max_active_tasks=3, max_active_runs=1, tags=["Test", "My second dag"])

task1 = BashOperator(  # Создание экземпляра BashOperator
    task_id='task1',  # Уникальный идентификатор задачи
    bash_command='python3 /airflow/scripts/dag2/task1.py --date {{ ds }} ' + f'--host {connection.host} --dbname {connection.schema} --user {connection.login} --jdbc_password {connection.password} --port 5432',
    # Команда для выполнения:
    # - Запускает скрипт Python
    # - Передает дату выполнения через {{ ds }}
    # - Указывает хост, имя базы данных, пользователя и пароль из объекта подключения
    dag=dag  # Указывает DAG, к которому принадлежит задача
)

task2 = BashOperator(
    task_id='task2',
    bash_command='python3 /airflow/scripts/dag2/task2.py',
    dag=dag)

task1 >> task2
