from datetime import datetime
from airflow.models import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.bash import BashOperator
from utils.check_table_sensor import CheckTableSensor

connection = BaseHook.get_connection("postgres_connection_main")

default_args = {
    "owner": "etl_user",
    "depends_on_past": False,
    "start_date": datetime(2024, 9, 28),
}

dag = DAG('dag5', default_args=default_args, schedule_interval='0 * * * *', catchup=True,
          max_active_tasks=3, max_active_runs=1, tags=["Test"])

task1 = BashOperator(
    task_id='task1',
    bash_command='python3 /airflow/scripts/dag5/task1.py',
    dag=dag)

# Создаем экземпляр класса CheckTableSensor и сохраняем в переменной task_check_table_sensor
task_check_table_sensor = CheckTableSensor(
    task_id=f'task_check_table_sensor',  # Уникальный идентификатор задачи в Airflow
    timeout=1000,  # Время ожидания выполнения задачи в секундах (максимальное время, в течение которого будет попытка проверки)
    mode='reschedule',  # Режим работы сенсора; 'reschedule' означает, что сенсор будет возвращаться к состоянию ожидания после каждой проверки
    poke_interval=10,  # Интервал между проверками в секундах (через сколько секунд снова будет выполнен метод poke)
    conn=connection,  # Передаем объект подключения к базе данных, который используется в классе CheckTableSensor
    table_name='op_table',  # Имя таблицы в базе данных, который будет проверяться на наличие данных
    dag=dag  # Передаем экземпляр DAG (Directed Acyclic Graph), который содержит задачи и их зависимости
)

task3 = BashOperator(
    task_id='task3',
    bash_command='python3 /airflow/scripts/dag_sensor/task3.py',
    dag=dag)

for i in [1, 2, 3, 4, 5]:
    some_task = BashOperator(
    task_id=f'task4_{str(i)}',
    bash_command='python3 /airflow/scripts/dag_sensor/task1.py',
    dag=dag)
    task3 >> some_task

task1 >> task_check_table_sensor >> task3
